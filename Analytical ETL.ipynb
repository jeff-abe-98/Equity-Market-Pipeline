{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75fd867e-142d-4690-88b2-32d7606ccec7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DecimalType, StringType, DateType, TimestampType\n",
    "from pyspark.sql import functions as F\n",
    "from decimal import Decimal\n",
    "from datetime import datetime\n",
    "spark = SparkSession.builder.appName('AnalyticalETL').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83508dc4-b9f9-48b9-b060-0299efd46ade",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet('/mnt/loaded/trade').write.mode('overwrite').saveAsTable('trades')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1059f02-a0d8-4d4c-bf2e-7382a3be8d99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trade_date = '2020-08-06'\n",
    "# in real runtime this would be datetime.today().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f0dabaf-a59f-46d1-8718-7e7c7f9da13f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(f\"select trade_dt, symbol, exchange, event_tm, event_seq_nb, trade_pr from trades where trade_dt = '{trade_date}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8681931-fd0e-4dcf-a4b0-d820258e057a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"tmp_trade_moving_avg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35b62cec-8f68-496e-99e1-607dd8a17415",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mov_avg_df = spark.sql(\"\"\"\n",
    "select main.trade_dt, main.symbol, main.exchange, main.event_tm, main.event_seq_nb, main.trade_pr,\n",
    "(select avg(trade_pr) from tmp_trade_moving_avg as sub where (unix_timestamp(main.event_tm) - unix_timestamp(sub.event_tm))/60 <= 30 and main.symbol=sub.symbol) as mov_avg_pr\n",
    "from tmp_trade_moving_avg as main\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca3be652-a4f4-4bec-8151-1b2df16833fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mov_avg_df.write.mode('overwrite').option('mergeSchema', 'true').saveAsTable(\"temp_trade_moving_avg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "814ae10b-6c5f-4ca0-837a-151fb2ad070b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "date = datetime.datetime.strptime(trade_date, '%Y-%m-%d')\n",
    "prev_date_str = (date - datetime.timedelta(days=1)).strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4705536c-918b-4763-a657-abe61f729612",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(f\"select trade_dt, symbol, exchange, event_tm, event_seq_nb, trade_pr from trades where trade_dt = '{prev_date_str}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5da42036-468f-4484-9981-5428dffeee92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"tmp_last_trade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ab99c26-b6df-4236-bd75-ab7ececfef18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "last_pr_df = spark.sql(\"\"\"select trade_dt, symbol, exchange, last_pr from (select\n",
    "trade_dt, symbol, exchange, event_tm, event_seq_nb, trade_pr, \n",
    "(select avg(trade_pr) from tmp_last_trade as sub where (unix_timestamp(main.event_tm) - unix_timestamp(sub.event_tm))/60 <= 30 and main.symbol=sub.symbol HAVING MAX(event_tm) = event_tm) AS last_pr\n",
    "FROM tmp_last_trade AS main) a\n",
    "WHERE last_pr IS NOT NULL\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b129cdee-6a9c-4b63-935f-f5bc94854422",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "last_pr_df.write.mode('overwrite').option('mergeSchema', 'true').saveAsTable(\"temp_last_trade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1e3b7ef-86fd-4be1-a452-a2349c572112",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.parquet('/mnt/loaded/quote').filter(F.col('trade_dt') == trade_date).write.mode('overwrite').saveAsTable('quotes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa9a3cb4-d5be-4794-9cfc-1c89f57554b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "quote_union = spark.sql(\"\"\"SELECT * FROM \n",
    "                                (SELECT trade_dt, \"T\" AS rec_type, symbol, event_tm, NULL AS event_seq_nb, exchange, NULL AS bid_pr, NULL AS bid_size, NULL AS ask_pr, NULL AS ask_size, trade_pr, mov_avg_pr\n",
    "                                FROM temp_trade_moving_avg)\n",
    "                                UNION\n",
    "                                (SELECT trade_dt, \"Q\" AS rec_type, symbol, event_tm, event_seq_nb, exchange, bid_pr, bid_size, ask_pr, ask_size, NULL AS trade_pr, NULL AS mov_avg_pr\n",
    "                                FROM quotes)\n",
    "                        \"\"\")\n",
    "quote_union.createOrReplaceTempView(\"quote_union\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b33d990-07a1-4cc7-acf9-08ffcf0f2697",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "quote_union_update = spark.sql(\"\"\"\n",
    "                               SELECT *,\n",
    "                               first_value(trade_pr, true) OVER(PARTITION BY symbol ORDER BY event_tm DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS last_trade_pr,\n",
    "                               first_value(mov_avg_pr, true) OVER(PARTITION BY symbol ORDER BY event_tm DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW ) AS last_mov_avg_pr\n",
    "                               FROM quote_union\n",
    "                               \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbea0019-9bd3-44cc-b96a-c2c7d97abd56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "quote_union_update.createOrReplaceTempView(\"quote_union_update\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0acc4193-6c24-4b27-bd89-e2d18b5262a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "quote_update = spark.sql(\"\"\"\n",
    "select trade_dt, symbol, event_tm, event_seq_nb, exchange,\n",
    "bid_pr, bid_size, ask_pr, ask_size, quote_union_update.last_trade_pr, quote_union_update.last_mov_avg_pr\n",
    "from quote_union_update\n",
    "where rec_type = 'Q'\n",
    "\"\"\")\n",
    "quote_update.createOrReplaceTempView(\"quote_update\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bdf2ba1-31c0-4d99-99aa-86bffd7ce99f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "quote_final = spark.sql('''\n",
    "                        select trade_dt, symbol, event_tm, event_seq_nb, exchange,\n",
    "                    bid_pr, bid_size, ask_pr, ask_size, last_trade_pr, last_mov_avg_pr,\n",
    "                    bid_pr - close_pr as bid_pr_mv, ask_pr - close_pr as ask_pr_mv\n",
    "                    from (\n",
    "                        SELECT /*+ BROADCAST(t) */ q.*, t.last_pr AS close_pr\n",
    "                        FROM\n",
    "                        quote_update q LEFT OUTER JOIN temp_last_trade t ON q.symbol = t.symbol\n",
    "                        )\n",
    "                        ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a54ce70f-a444-41a6-a3a1-6e35eb17d8f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "quote_final.write.mode('overwrite').partitionBy('trade_dt').parquet('/mnt/quote-trade-analytical')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Analytical ETL",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
